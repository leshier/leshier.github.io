<!DOCTYPE html>
<html lang="zh">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="renderer" content="webkit">
    <link rel="icon" href="data:;base64,=">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
    <title>
        leshier&#39;s Blog
    </title>
    
<link rel="stylesheet" href="/libs/highlight/styles/monokai-sublime.css">

    
<link rel="stylesheet" href="/libs/font-awesome/css/font-awesome.min.css">

    
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 5.2.0"></head>

<body id="bodyx">
    
<div class="hd posts">
    <a href="/index.html"><i class="fa fa-reply replay-btn" aria-hidden="true"></i></a>
    <div class="post-title">
        <p>
            
        </p>
        <hr>
    </div>

    <div class="post-content">
        <h1 id="Reviewer-1"><a href="#Reviewer-1" class="headerlink" title="Reviewer 1"></a>Reviewer 1</h1><p>1、添加两个相近的论文的结果<br>[1] Zero-Reference Deep Curve Estimation for Low-Light Image Enhancement, Guo et al.</p>
<p>​    <a target="_blank" rel="noopener" href="https://li-chongyi.github.io/Proj_Zero-DCE.html">https://li-chongyi.github.io/Proj_Zero-DCE.html</a> <code>Code</code> </p>
<p>[2] Deep Bilateral Learning for Real-Time Image Enhancements, Gharbi et al.</p>
<p>​    <a target="_blank" rel="noopener" href="https://groups.csail.mit.edu/graphics/hdrnet/">https://groups.csail.mit.edu/graphics/hdrnet/</a> <code>Code</code> </p>
<p>2、在公式(1)中没有定义’S’，但在第239行中引入了它。    </p>
<p>3、请添加用于训练和测试的HDR数据集的详细信息。</p>
<h1 id="Reviewer-2"><a href="#Reviewer-2" class="headerlink" title="Reviewer 2"></a>Reviewer 2</h1><p>1、整个方法依赖于NLPD指标在匹配感知方面的充分性。但最近的研究表明，NLPD在这方面的表现不尽如人意，在一些与它所优化的数据库完全不同的数据库中，NLPD与观察者的得分相关性一般或低于平均水平。<br>    Ding, K., Ma, K., Wang, S., &amp; Simoncelli, E. P. (2020). A Comparative Study of Image Quality Assessment Models through Perceptual Optimization. arXiv preprint arXiv:2005.01338.<br>2、我特别疑惑的是NLPD的第一步，在公式（1）中。这是一个幂律，作者说这是 “近似于光对视网膜光感受器的响应的转化”。这显然是错误的，光感受器的响应从来没有用幂律来建模，而是用Michaeles-Menten或Naka-Rushton方程来建模，例如见：<br>    Shapley, R., &amp; Enroth-Cugell, C. (1984). Visual adaptation and retinal gain controls. Progress in retinal research, 3, 263-346.<br>3、但对于音调映射来说，它的错误特别多：在音调映射界，用纳卡-鲁什顿方程模拟感光体反应能产生非常好的音调映射结果，而功率法则不能产生那么好的结果，这已经是20年前的事情了：<br>    Pattanaik, S. N., Tumblin, J., Yee, H., &amp; Greenberg, D. P. (2000, July). Time-dependent visual adaptation for fast realistic image display. In Proceedings of the 27th annual conference on Computer graphics and interactive techniques (pp. 47-54).<br>4、这就给我带来了另一个主要的批评：作者似乎对语气映射文献并不熟悉。这大概就是为什么他们会做出 “上述TMO的设计大多基于经验规则，这种规则的感性优化的有效性不大 “这样的言论，暗示（这是我的印象，对不起）他们通过展示如何将感性优化纳入音调映射方法的设计中，对该领域做出了重要贡献。然而事实是，感性优化是有史以来第一个提出的音调映射方法的核心，关于基于感性原理的音调映射有大量的文献，例如见第9章和其中的参考文献。<br>    Bertalmío, M. (2019). Vision Models for High Dynamic Range and Wide Colour Gamut Imaging: Techniques and Applications. Academic Press.<br>5、虽然我很欣赏作者进行了定量验证和主观实验，都把他们提出的方法排在了第一位，但从所有图中显示的所有例子来看，该方法的结果由于过度增强对比度而出现了不自然的外观，出现了非常令人心烦的伪影。这些结果根本不像是照片，而像是经过处理的照片，经过人为的增强，在我的眼里，它们显然不如普通相机拍出来的效果。我猜想，如果主观实验中不仅包括色调映射方法的结果，还包括照片相机的最佳单次曝光结果，那么排名就会不一样，照片相机的结果会排在第一位，例如见：<br>    Narwaria, M., Da Silva, M. P., Le Callet, P., &amp; Pepion, R. (2014, September). Single exposure vs tone mapped high dynamic range images: a study based on quality of experience. In 2014 22nd European Signal Processing Conference (EUSIPCO) (pp. 2140-2144). IEEE.<br>6、对非标定输入的处理，这是目前最常见的情况，似乎有点临时性。</p>
<h1 id="Reviewer-3"><a href="#Reviewer-3" class="headerlink" title="Reviewer 3"></a>Reviewer 3</h1><p>1、该方法是NLPD的直接应用，没有太多新的技术贡献。训练CNN的方法（S1）很有意思，但作者并没有试图解释为什么他们的方法比直接优化（NLPD-Opt）产生更好的结果。直觉上，情况应该是相反的。<br>2、论文中显示的和用于训练的图像非常小（较大的边缘有512个像素）。该方法能否处理较大的图像，还是只适合处理缩略图？该方法能否处理不同大小的图像，并仍然产生良好的结果？(请在反驳中解决这些问题)</p>
<p>​    加上分辨率的对比实验</p>
<p>![image-20210119140545786](/Users/leshier/Library/Application Support/typora-user-images/image-20210119140545786.png)</p>
<p>![image-20210119140629782](/Users/leshier/Library/Application Support/typora-user-images/image-20210119140629782.png)</p>
<p>3、关于生物启发音调映射的说法在论文中没有理由，也不是这项工作的贡献（应该从标题和摘要中删除）。唯一受生物启发的部分是NLPD度量，这不是提交论文的贡献。论文甚至没有解释NLPD占了哪个视觉机制。<br>4、看到主观质量评价实验的结果是好的，但我担心的是，结果可能有偏差。作者写道：”我们选择了20张HDR图像”，这可能表明选择不是随机和盲目的，因此它可能会偏向于那些所提出的方法效果好的图像。<br>5、虽然该方法的主要优点是处理时间快，但作者并没有比较所有测试方法的性能（对于不同分辨率的图像）。<br>6、我熟悉这项工作中所比较的大多数算子，我怀疑有些算子（Dargo03、Reinhard05、WLS）的结果是错误产生的。Dargo03和Reinhard05使用了错误的伽玛，WLS选择了错误的剪切点。这些运算符在正确使用时不会产生低对比度和过曝的图像。<br>7、测试组中缺少暗图像。</p>
<p>主观实验需要重新做</p>

    </div>

    <div>
        
    <section>
      <div id="gitalk-container"></div>
    </section>

        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
<script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
<script type="text/javascript">
    var gitalk = new Gitalk({
        clientID: 'ed09c837cb5a287364c2',
        clientSecret: '772e6894e16520ef6e42a457144c463f11882870',
        repo: 'leshier.github.io',
        owner: 'leshier',
        admin: 'leshier',
        id: location.pathname,      // Ensure uniqueness and length less than 50
        distractionFreeMode: 'false'  // Facebook-like distraction free mode
    })
    gitalk.render('gitalk-container')
</script>
    </div>
</div>

    <div class="footer" id="footer">
    
        
<script src="/js/busuanzi.pure.mini.js"></script>

        <span id="busuanzi_container_site_pv">Site PV: <span id="busuanzi_value_site_pv"></span></span>
        <span class="post-meta-divider">|</span>
        <span id="busuanzi_container_site_uv">Site UV: <span id="busuanzi_value_site_uv"></span></span>
    
    <p>Copyright © 2020 <a class="flink" href="#">Modified Geek by leshier</a>.
        <label class="el-switch el-switch-green el-switch-sm" style="vertical-align: sub;">
            <input type="checkbox" name="switch" id="update_style">
            <span class="el-switch-style"></span>
        </label>
        
    </p>
</div>
<input type="hidden" id="web_style" value="black"> 


<script src="/libs/jquery.min.js"></script>


<script src="/libs/highlight/highlight.pack.js"></script>


<script src="/js/js.js"></script>


</body>

</html>